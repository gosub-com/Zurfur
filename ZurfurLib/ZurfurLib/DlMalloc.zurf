// The MIT License (MIT)
//
// Copyright (c) 2019 by Jeremy Spiller
//
// Permission is hereby granted, free of charge, to any person obtaining a
// copy of this software and associated documentation files (the "Software"),
// to deal in the Software without restriction, including without limitation
// the rights to use, copy, modify, merge, publish, distribute, sublicense,
// and/or sell copies of the Software, and to permit persons to whom the
// Software is furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included
// in all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
// FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
// DEALINGS IN THE SOFTWARE.

//  This is a version (aka dlmalloc) of malloc/free/realloc written by
//  Doug Lea and released to the public domain, as explained at
//  http://creativecommons.org/publicdomain/zero/1.0/ Send questions,
//  comments, complaints, performance data, etc to dl@cs.oswego.edu
//
//* Version 2.8.6 Wed Aug 29 06:57:58 2012  Doug Lea
//   Note: There may be an updated version of this malloc obtainable at
//           ftp://gee.cs.oswego.edu/pub/misc/malloc.c

mod Zurfur.Internal.Memory

use Zurfur.Unsafe[Xuint, castPointer, RawPointer]

fun panicFatal(m Str)
    todo
    
// Windows allocation function (TBD: Remove this)
fun AllocHGlobal(size U64) *Void
    todo

// This was originally CALL_MMAP and is used to get more memory from
// the system. It's similar to CALL_MMAP, but the function can increase
// the size of allocated memory if desired  (must be in units of page size).
// Returns nil when out of memory or on error.
// TBD: This returned the length
fun CallMoreCore(my DlMalloc, length mut &U64) *Void
    todo

// This was originally CALL_MUNMAP and is used to release memory pages
// back to the OS.  This function works with pages, not with units
// individually allocated by CallMoreCore.  Therefore it could try to
// free part of an allocated memory block or even multiple blocks at a time.
// Return true for success, or false for failure (in which
// case the pages are retained and used for future requests)
fun CallReleaseCore(my DlMalloc, address *Void, length U64) Bool
    todo

// Called when the heap is known to be corrupted.  Throws an exception
// and calls ResetOnError by default.
fun CallCorruptionErrorAction(my DlMalloc)
    panicFatal("Malloc Corrupted: TBD: Give more info about address")

// Called when the malloc is used incorrectly.  Throws an exception
// and calls ResetOnError by default.
fun CallUsageErrorAction(my DlMalloc, m2 *Void)
    panicFatal("Malloc Usage Error: TBD: Give more info about address")

// Called before malloc returns nil.  The default action is to throw an exception
fun CallMallocFailureAction(my DlMalloc)
    panicFatal("Malloc out of memory")

// Bin types, widths and sizes
const NSMALLBINS U64 = 32
const NTREEBINS U64 = 32
const SMALLBIN_SHIFT U64 = 3
const SMALLBIN_WIDTH U64 = SIZE_T_ONE << SMALLBIN_SHIFT
const TREEBIN_SHIFT U64 = 8U
const MIN_LARGE_SIZE U64 = SIZE_T_ONE << TREEBIN_SHIFT
const MAX_SMALL_SIZE U64 = MIN_LARGE_SIZE - SIZE_T_ONE
fun get MAX_SMALL_REQUEST() U64
    return MAX_SMALL_SIZE - CHUNK_ALIGN_MASK - CHUNK_OVERHEAD


[pub] type unsafe DlMalloc

    //  Top
    //    The topmost chunk of the currently active segment. Its size is
    //    cached in topsize.  The actual size of topmost space is
    //    topsize+TOP_FOOT_SIZE, which includes space reserved for adding
    //    fenceposts and segment records if necessary when getting more
    //    space from the system.  The size at which to autotrim top is
    //    cached from mparams in trim_check, except that it is disabled if
    //    an autotrim fails.

    //  Designated victim (dv)
    //    This is the preferred chunk for servicing small requests that
    //    don't have exact fits.  It is normally the chunk split off most
    //    recently to service another small request.  Its size is cached in
    //    dvsize. The link fields of this chunk are not maintained since it
    //    is not kept in a bin.

    //  SmallBins
    //    An array of bin headers for free chunks.  These bins hold chunks
    //    with sizes less than MIN_LARGE_SIZE bytes. Each bin contains
    //    chunks of all the same size, spaced 8 bytes apart.  To simplify
    //    use in double-linked lists, each bin header acts as a malloc_chunk
    //    pointing to the real first node, if it exists (else pointing to
    //    itself).  This avoids special-casing for headers.  But to avoid
    //    waste, we allocate only the fd/bk pointers of bins, and then use
    //    repositioning tricks to treat these as the fields of a chunk.

    //  TreeBins
    //    Treebins are pointers to the roots of trees holding a range of
    //    sizes. There are 2 equally spaced treebins for each power of two
    //    from TREE_SHIFT to TREE_SHIFT+16. The last bin holds anything
    //    larger.

    //  Bin maps
    //    There is one bit map for small bins ("smallmap") and one for
    //    treebins ("treemap).  Each bin sets its bit when non-empty, and
    //    clears the bit when empty.  Bit operations are then used to avoid
    //    bin-by-bin searching -- nearly all "search" is done without ever
    //    looking at bins that won't be selected.  The bit maps
    //    conservatively use 32 bits per map word, even if on 64bit system.
    //    For a good description of some of the bit-based techniques used
    //    here, see Henry S. Warren Jr's book "Hacker's Delight" (and
    //    supplement at http://hackersdelight.org/). Many of these are
    //    intended to reduce the branchiness of paths through malloc etc, as
    //    well as to reduce the number of memory locations read or written.

    //  Segments
    //    A list of segments headed by an embedded malloc_segment record
    //    representing the initial space.

    //  Address check support
    //    The least_addr field is the least address ever obtained from
    //    MORECORE or MMAP. Attempted frees and reallocs of any address less
    //    than this are trapped

    //  Magic tag
    //    A cross-check field that should always hold same value as mparams.magic.

    //  Max allowed footprint
    //    The maximum allowed bytes to allocate from system (zero means no limit)

    //  Flags
    //    Bits recording whether to use MMAP or contiguous MORECORE

    //  Statistics
    //    Each space keeps track of current and maximum system memory
    //    obtained via MORECORE or MMAP.

    //  Trim support
    //    Fields holding the amount of unused topmost memory that should trigger
    //    trimming, and a counter to force periodic scanning to release unused
    //    non-topmost segments.

    smallmap U64
    treemap U64
    dvsize U64
    topsize U64
    least_addr *Byte
    dv *mchunk
    top *mchunk
    trim_check U64
    release_checks U64
    magic U64
    smallbins **mchunk
    treebins **tchunk
    footprint U64
    max_footprint U64
    seg *msegment
    malloc_corruption_error_count Int
    mparams malloc_params

fun new(my DlMalloc, capacity U64)
    my.ensure_initialization()
    let rs = ife capacity == 0 : my.mparams.granularity : capacity + TOP_FOOT_SIZE
    let tsize = my.granularity_align(rs)
    let tbase = castPointer<*Byte>(my.CallMoreCore(&tsize))
    if tbase != nil
        my.init_user_mstate(tbase, tsize)
    else
        panicFatal("Out of memory, cannot initialize malloc.")

// Returns the number of malloc segments, which represents
// the number of discontiguous memory regions.
fun get SegmentCount(my DlMalloc) Int
    let count = 0
    let segment = my.seg
    while segment != nil
        count += 1
        segment = segment.next
    return count

//  Supported pointer/size_t representation:       4 or 8 bytes
//  Alignment:                                     8 bytes (minimum)
//       This suffices for nearly all current machines and C compilers.
//       However, you can define MALLOC_ALIGNMENT to be wider than this
//       if necessary (up to 128bytes), at the expense of using more space.
const MALLOC_ALIGNMENT U64 = 8

//  Minimum overhead per allocated chunk:   4 bytes (if 4 Byte pointer sizes)
//                                          8 bytes (if 8 Byte pointer sizes)

//  Minimum allocated size: 4-Byte ptrs:  16 bytes    (including overhead)
//                          8-Byte ptrs:  32 bytes    (including overhead)

//       Even a request for zero bytes (i.e., malloc(0)) returns a
//       pointer to something of the minimum allocatable size.
//       The maximum overhead wastage (i.e., number of extra bytes
//       allocated than were requested in malloc) is less than or equal
//       to the minimum size

//  Thread-safety: NOT thread-safe

// Overview of algorithms

//  In most ways, this malloc is a best-fit allocator. Generally, it
//  chooses the best-fitting existing chunk for a request, with ties
//  broken in approximately least-recently-used order. (This strategy
//  normally maintains low fragmentation.) However, for requests less
//  than 256bytes, it deviates from best-fit when there is not an
//  exactly fitting available chunk by preferring to use space adjacent
//  to that used for the previous small request, as well as by breaking
//  ties in approximately most-recently-used order. (These enhance
//  locality of series of small allocations.)

//  All operations (except malloc_stats and mallinfo) have execution
//  times that are bounded by a constant factor of the number of bits in
//  a size_t, not counting any clearing in calloc or copying in realloc,
//  or actions surrounding MORECORE and MMAP that have times
//  proportional to the number of non-contiguous regions returned by
//  system allocation routines, which is often just 1. In real-time
//  applications, you can optionally suppress segment traversals using
//  NO_SEGMENT_TRAVERSAL, which assures bounded execution even when
//  system allocators return non-contiguous spaces, at the typical
//  expense of carrying around more memory and increased fragmentation.

//  For a longer but out of date high-level description, see
//     http://gee.cs.oswego.edu/dl/html/malloc.html


// NO_SEGMENT_TRAVERSAL       default: 0
//  If non-zero, suppresses traversals of memory segments
//  returned by CALL_MMAP. This disables
//  merging of segments that are contiguous, and selectively
//  releasing them to the OS if unused, but bounds execution times.

const NO_SEGMENT_TRAVERSAL Bool = false

// DEFAULT_GRANULARITY        default: page size (64K)
//  The unit for allocating and deallocating memory from the system.  On
//  most systems with contiguous MORECORE, there is no reason to
//  make this more than a page. However, systems with MMAP tend to
//  either require or encourage larger granularities.  You can increase
//  this value to prevent system allocation functions to be called so
//  often, especially if they are slow.  The value must be at least one
//  page and must be a power of two.  Setting to 0 causes initialization
//  to either page size or win32 region size.  (Note: In previous
//  versions of malloc, the equivalent of this option was called
//  "TOP_PAD")
const DEFAULT_GRANULARITY U64 = 65536

// DEFAULT_TRIM_THRESHOLD    default: 2MB
//      Also settable using mallopt(M_TRIM_THRESHOLD, x)
//  The maximum amount of unused top-most memory to keep before
//  releasing via malloc_trim in free().  Automatic trimming is mainly
//  useful in long-lived programs using contiguous MORECORE.  Because
//  trimming via sbrk can be slow on some systems, and can sometimes be
//  wasteful (in cases where programs immediately afterward allocate
//  more large chunks) the value should be high enough so that your
//  overall system performance would improve by releasing this much
//  memory.  As a rough guide, you might set to a value close to the
//  average size of a process (program) running on your system.
//  Releasing this much memory would allow such a process to run in
//  memory.  Generally, it is worth tuning trim thresholds when a
//  program undergoes phases where several large chunks are allocated
//  and released in ways that can reuse each other's storage, perhaps
//  mixed with phases where there are no such chunks at all. The trim
//  value must be greater than page size to have any useful effect.  To
//  disable trimming completely, you can set to MAX_SIZE_T. Note that the trick
//  some people use of mallocing a huge space and then freeing it at
//  program startup, in an attempt to reserve system memory, doesn't
//  have the intended effect under automatic trimming, since that memory
//  will immediately be returned to the system.
const DEFAULT_TRIM_THRESHOLD U64 = 2 * 1024 * 1024


// MAX_RELEASE_CHECK_RATE   default: 4095
//  The number of consolidated frees between checks to release
//  unused segments when freeing. When using non-contiguous segments,
//  especially with multiple mspaces, checking only for topmost space
//  doesn't always suffice to trigger trimming. To compensate for this,
//  free() will, with a period of MAX_RELEASE_CHECK_RATE (or the
//  current number of segments, if greater) try to release unused
//  segments to the OS when freeing chunks that result in
//  consolidation. The best value for this parameter is a compromise
//  between slowing down frees with relatively costly checks that
//  rarely trigger versus holding on to unused memory. To effectively
//  disable, set to MAX_SIZE_T. This may lead to a very slight speed
//  improvement at the expense of carrying around more memory.
const MAX_RELEASE_CHECK_RATE U64 = 4095


// The maximum possible U64 value has all bits set
const MAX_SIZE_T U64 = xunit.MaxValue

// ------------------- U64 and alignment properties --------------------

// The Byte and bit size of a U64
fun get SIZE_T_SIZE() U64
    return sizeof(U64).toU64Mask
fun get SIZE_T_BITSIZE() U64
    return (sizeof(U64) << 3).toU64Mask

// Some constants coerced to size_t
const SIZE_T_ZERO U64 = 0
const SIZE_T_ONE U64 = 1
const SIZE_T_TWO U64 = 2
const SIZE_T_FOUR U64 = 4
fun get TWO_SIZE_T_SIZES() U64
    return SIZE_T_SIZE << 1
fun get FOUR_SIZE_T_SIZES() U64
    return SIZE_T_SIZE << 2

// The bit mask value corresponding to MALLOC_ALIGNMENT
const CHUNK_ALIGN_MASK U64 = MALLOC_ALIGNMENT - SIZE_T_ONE

// True if address a has acceptable alignment
fun is_aligned(a *Void) Bool
    return (castPointer<U64>(a) & CHUNK_ALIGN_MASK) == 0

// the number of bytes to offset an address to align it
fun align_offset(a *Void) U64
    if (castPointer<U64>(a) & CHUNK_ALIGN_MASK) == 0
        return 0U64
    return (MALLOC_ALIGNMENT - (castPointer<U64>(a) & CHUNK_ALIGN_MASK)) & CHUNK_ALIGN_MASK

// Ensure mparams initialized
fun ensure_initialization(my DlMalloc)
    if my.mparams.magic == 0
        my.init_mparams()

fun is_initialized(my DlMalloc) Bool
    return my.top != nil

// -------------------------- system alloc setup -------------------------

// Operations on mflags

// page-align a size
fun page_align(my DlMalloc, s U64) U64
    return (s + (my.mparams.page_size - SIZE_T_ONE)) & ~(my.mparams.page_size - SIZE_T_ONE)

// granularity-align a size
fun granularity_align(my DlMalloc, s U64) U64
    return ((s) + (my.mparams.granularity - SIZE_T_ONE)) & ~(my.mparams.granularity - SIZE_T_ONE)

fun mmap_align(my DlMalloc, s U64) U64
    return my.granularity_align(s)

// For sys_alloc, enough padding to ensure can malloc request on success
fun get SYS_ALLOC_PADDING() U64
    return TOP_FOOT_SIZE + MALLOC_ALIGNMENT

fun is_page_aligned(my DlMalloc, s U64) Bool
    return (s & (my.mparams.page_size - SIZE_T_ONE)) == 0

fun is_granularity_aligned(my DlMalloc, s U64) Bool
    return (s & (my.mparams.granularity - SIZE_T_ONE)) == 0

//  True if segment S holds address A
fun segment_holds(my DlMalloc, s *msegment, a *Void) Bool
    return a >= castPointer<*Void>(s.baseAddr) and a < castPointer<*Void>(s.baseAddr + s.size)

// Return segment holding given address
fun segment_holding(my DlMalloc, addr *Byte) *msegment
    let sp = my.seg
    while true
        if addr >= sp.baseAddr and addr < sp.baseAddr + sp.size
            return sp
        sp = sp.next
        if sp == nil
            return nil

// Return true if segment contains a segment link
fun has_segment_link(my DlMalloc, ss *msegment) Bool
    let sp = my.seg
    while true
        if castPointer<*Byte>(sp) >= ss.baseAddr and castPointer<*Byte>(sp) < ss.baseAddr + ss.size
            return true
        sp = sp.next
        if sp == nil
            return false

fun should_trim(my DlMalloc, s U64) Bool
    return s > my.trim_check

//  TOP_FOOT_SIZE is padding at the end of a segment, including space
//  that may be needed to place segment records and fenceposts when new
//  noncontiguous segments are added.
fun get TOP_FOOT_SIZE() U64
    return align_offset(chunk2mem(castPointer<*mchunk>(Xuint(0 U64)))) + pad_request(sizeof(msegment).toU64Mask) + MIN_CHUNK_SIZE

// -------------------------- Debugging setup ----------------------------

[Conditional("DEBUG")]
fun check_free_chunk(my DlMalloc, p *mchunk)
    my.do_check_free_chunk(p)

[Conditional("DEBUG")]
fun check_inuse_chunk(my DlMalloc, p *mchunk)
    my.do_check_inuse_chunk(p)

[Conditional("DEBUG")]
fun check_malloced_chunk(my DlMalloc, mem *Void, s U64)
    my.do_check_malloced_chunk(mem, s)

[Conditional("DEBUG")]
fun check_top_chunk(my DlMalloc, p *mchunk)
    my.do_check_top_chunk(p)

// ---------------------------- Indexing Bins ----------------------------

fun is_small(s U64) Bool
    return (s >> SMALLBIN_SHIFT) < NSMALLBINS
fun small_index(s U64) U64
    return s >> SMALLBIN_SHIFT
fun small_index2size(i U64) U64
    return i << SMALLBIN_SHIFT
fun get MIN_SMALL_INDEX() U64
    return small_index(MIN_CHUNK_SIZE)

// addressing by index. See above about smallbin repositioning
fun smallbin_at(my DlMalloc, i U64) *mchunk
    return castPointer<*mchunk>(my.smallbins + i*2)
fun treebin_at(my DlMalloc, i U64) **tchunk
    return castPointer<**tchunk>(my.treebins + i)

// Find tree index for size S
fun compute_tree_index(s U64) U64
    let x = s >> TREEBIN_SHIFT
    if x == 0
        return 0 U64
    if x > 0xFFFF
        return NTREEBINS - 1U64
    let y = x
    let n = ((y - 0x100U64) >> 16) & 8
    y <<= n
    let k = ((y - 0x1000U64) >> 16) & 4
    n += k
    y <<= k
    k = ((y - 0x4000U64) >> 16) & 2
    n += k
    y <<= k
    k = 14U64 - n + (y >> 15)
    return (k << 1) + ((s >> (k + TREEBIN_SHIFT - 1U64)) & 1)


// Bit representing maximum resolved size in a treebin at i
fun bit_for_tree_index(i U64) U64
    if i == NTREEBINS - 1U64
        return SIZE_T_BITSIZE - 1U64
    return (i >> 1) + TREEBIN_SHIFT - 2U64

// Shift placing maximum resolved bit in a treebin at i as sign bit
fun leftshift_for_tree_index(i U64) Int
    if i == NTREEBINS - 1U64
        return 0 Int
    return (SIZE_T_BITSIZE - SIZE_T_ONE - ((i >> 1) + TREEBIN_SHIFT - 2U64)).toIntMask

// The size of the smallest chunk held in bin with index i
fun minsize_for_tree_index(i U64) U64
    return SIZE_T_ONE << ((i >> 1) + TREEBIN_SHIFT)
            | ((i & SIZE_T_ONE)) << ((i >> 1) + TREEBIN_SHIFT - 1U64)

// ------------------------ Operations on bin maps -----------------------

// bit corresponding to given index
fun idx2bit(i U64) U64
    return 1U64 << i

// Mark/Clear bits with given index
fun mark_smallmap(my mut DlMalloc, i U64) U64
    my.smallmap |= idx2bit(i)
    return my.smallmap
    
fun clear_smallmap(my mut DlMalloc, i U64) U64
    my.smallmap &= ~idx2bit(i)
    return my.smallmap
    
fun smallmap_is_marked(my DlMalloc, i U64) Bool
    return (my.smallmap & idx2bit(i)) != 0
    
fun mark_treemap(my mut DlMalloc, i U64) U64
    my.treemap |= idx2bit(i)
    return my.treemap
    
fun clear_treemap(my mut DlMalloc, i U64) U64
    my.treemap &= ~idx2bit(i)
    return my.treemap
    
fun treemap_is_marked(my DlMalloc, i U64) Bool
    return (my.treemap & idx2bit(i)) != 0

// isolate the least set bit of a bitmap
fun least_bit(x U64) U64
    return x & (-x)

// mask with all bits to left of least bit of x on
fun left_bits(x U64) U64
    return (x << 1) | (-(x << 1))

// mask with all bits to left of or equal to least bit of x on
fun same_or_left_bits(x U64) U64
    return x | (-x)

// index corresponding to given bit.
fun compute_bit2idx(x U64) U64
    let y = x - 1U64
    let k = y >> (16 - 4) & 16
    let n = k
    y >>= k
    k = y >> (8 - 3) & 8
    n += k
    y >>= k
    k = y >> (4 - 2) & 4
    n += k
    y >>= k
    k = y >> (2 - 1) & 2
    n += k
    y >>= k
    k = y >> (1 - 0) & 1
    n += k
    y >>= k
    return n + y



// ----------------------- Runtime Check Support -------------------------

//  For security, the main invariant is that malloc/free/etc never
//  writes to a static address other than malloc_state, unless static
//  malloc_state itself has been corrupted, which cannot occur via
//  malloc (because of these checks). In essence this means that we
//  believe all pointers, sizes, maps etc held in malloc_state, but
//  check all of those linked or offsetted from other embedded data
//  structures.  These checks are interspersed with main code in a way
//  that tends to minimize their run-time cost.

//  In addition to range checking, we also [...]
//  always dynamically check addresses of all offset chunks (previous,
//  next, etc). This turns out to be cheaper than relying on hashes.


// Check if address a is at least as high as any from MORECORE or MMAP
fun ok_address(my DlMalloc, a *Void) Bool
    return a >= castPointer<*Void>(my.least_addr)
        
fun ok_next(p *tchunk, n *mchunk) Bool
    return castPointer<*Byte>(p) < castPointer<*Byte>(n)
fun ok_next(p *mchunk, n *mchunk) Bool
    return castPointer<*Byte>(p) < castPointer<*Byte>(n)
fun ok_inuse(p *mchunk) Bool
    return is_inuse(p)
fun ok_pinuse(p *mchunk) Bool
    return pinuse(p)


fun ok_magic(my DlMalloc) Bool
    return my.magic == my.mparams.magic

fun RTCHECK(e Bool) Bool
    return e

fun set_inuse_and_pinuse(p mut *mchunk, s U64)
    p.head = s | PINUSE_BIT | CINUSE_BIT
    (castPointer<*mchunk>(castPointer<*Byte>(p) + s)).head |= PINUSE_BIT

fun set_inuse_and_pinuse(p mut *tchunk, s U64)
    p.head = s | PINUSE_BIT | CINUSE_BIT
    (castPointer<*mchunk>(castPointer<*Byte>(p) + s)).head |= PINUSE_BIT

fun set_size_and_pinuse_of_inuse_chunk(p mut *mchunk, s U64)
    p.head = s | PINUSE_BIT | CINUSE_BIT

// ---------------------------- setting mparams --------------------------

fun GetEnvironmentSystemPageSize() U64 { }
fun GetTimeNowNowTicks() U64 { }

// Initialize mparams
fun init_mparams(my DlMalloc) Int
    // Sanity-check configuration:
    // U64 must be unsigned and as wide as pointer type.
    // ints must be at least 4 bytes.
    // alignment must be at least 8.
    // Alignment, min chunk size, and page size must all be powers of 2.
    let psize = GetEnvironmentSystemPageSize()
    let gsize = ife DEFAULT_GRANULARITY != 0 : DEFAULT_GRANULARITY : psize
    if sizeof(size_t) < sizeof(*Byte)
            or MAX_SIZE_T < MIN_CHUNK_SIZE
            or sizeof(Int) < 4
            or MALLOC_ALIGNMENT < 8
            or (MALLOC_ALIGNMENT & (MALLOC_ALIGNMENT - SIZE_T_ONE)) != 0
            or (MCHUNK_SIZE & (MCHUNK_SIZE - SIZE_T_ONE)) != 0
            or (gsize & (gsize - SIZE_T_ONE)) != 0
            or (psize & (psize - SIZE_T_ONE)) != 0
        panicFatal("Abort: Sanity check failed")

    if my.mparams.magic == 0
        my.mparams.granularity = gsize
        my.mparams.page_size = psize
        my.mparams.trim_threshold = DEFAULT_TRIM_THRESHOLD
        let magic = GetTimeNowNowTicks() + 0x55555555U64
        magic |= 8U64    // ensure nonzero
        magic &= ~7U64   // improve chances of fault for bad values
        my.mparams.magic = magic
    return 1

// ------------------------- Debugging Support ---------------------------


// Check properties of any chunk, whether free, inuse, mmapped etc
fun do_check_any_chunk(my DlMalloc, p *mchunk)
    assert(is_aligned(chunk2mem(p)) or p.head == FENCEPOST_HEAD)
    assert(my.ok_address(p))

// Check properties of top chunk
fun do_check_top_chunk(my DlMalloc, p *mchunk)
    let sp = my.segment_holding(castPointer<*Byte>(p))
    let sz = p.head & ~INUSE_BITS // third-lowest bit can be set!
    assert(sp != nil)
    assert(is_aligned(chunk2mem(p)) or p.head == FENCEPOST_HEAD)
    assert(my.ok_address(p))
    assert(sz == my.topsize)
    assert(sz > 0)
    assert(sz == castPointer<U64>((sp.baseAddr + sp.size) - castPointer<*Byte>(p)) - TOP_FOOT_SIZE)
    assert(pinuse(p))
    assert(not pinuse(chunk_plus_offset(p, sz)))

// Check properties of inuse chunks
fun do_check_inuse_chunk(my DlMalloc, p *mchunk)
    my.do_check_any_chunk(p)
    assert(is_inuse(p))
    assert(next_pinuse(p))
    // If not pinuse and not mmapped, previous chunk has OK offset
    assert(pinuse(p) or next_chunk(prev_chunk(p)) == p)
    assert(not is_mmapped(p))

// Check properties of free chunks
fun do_check_free_chunk(my DlMalloc, p *mchunk)
    let sz = chunksize(p)
    let next = chunk_plus_offset(p, sz)
    my.do_check_any_chunk(p)
    assert(not is_inuse(p))
    assert(not next_pinuse(p))
    assert(not is_mmapped(p))
    if p != my.dv and p != my.top
        if sz >= MIN_CHUNK_SIZE
            assert((sz & CHUNK_ALIGN_MASK) == 0)
            assert(is_aligned(chunk2mem(p)))
            assert(next.prev_foot == sz)
            assert(pinuse(p))
            assert(next == my.top or is_inuse(next))
            assert(p.fd.bk == p)
            assert(p.bk.fd == p)
        else  // markers are always of size SIZE_T_SIZE
            assert(sz == SIZE_T_SIZE)

// Check properties of malloced chunks at the point they are malloced
fun do_check_malloced_chunk(my DlMalloc, mem *Void, s U64)
    if mem != nil
        let p = mem2chunk(mem)
        let sz = p.head & ~INUSE_BITS
        my.do_check_inuse_chunk(p)
        assert((sz & CHUNK_ALIGN_MASK) == 0)
        assert(sz >= MIN_CHUNK_SIZE)
        assert(sz >= s)
        // unless mmapped, size is less than MIN_CHUNK_SIZE more than request
        assert(sz < (s + MIN_CHUNK_SIZE))
        assert(not is_mmapped(p))

// Check a tree and its subtrees.
fun do_check_tree(my DlMalloc, t *tchunk)
    let head *tchunk = nil
    let u = t
    let tindex = t.index
    let tsize = chunksize(t)
    let idx = compute_tree_index(tsize)
    assert(tindex == idx)
    assert(tsize >= MIN_LARGE_SIZE)
    assert(tsize >= minsize_for_tree_index(idx))
    assert(idx == NTREEBINS - 1U64 or tsize < minsize_for_tree_index(idx + 1U64))

    do
        // traverse through chain of same-sized nodes
        my.do_check_any_chunk((castPointer<*mchunk>(u)))
        assert(u.index == tindex)
        assert(chunksize(u) == tsize)
        assert(not is_inuse(u))
        assert(not next_pinuse(u))
        assert(u.fd.bk == u)
        assert(u.bk.fd == u)
        if u.parent == nil
            assert(u.child0 == nil)
            assert(u.child1 == nil)
        else
            assert(head == nil) // only one node on chain has parent
            head = u
            assert(u.parent != u)
            assert(u.parent.child0 == u
                    or u.parent.child1 == u
                    or (castPointer<**tchunk>(u.parent)).* == u)
            if u.child0 != nil
                assert(u.child0.parent == u)
                assert(u.child0 != u)
                my.do_check_tree(u.child0)
            if u.child1 != nil
                assert(u.child1.parent == u)
                assert(u.child1 != u)
                my.do_check_tree(u.child1)
            if u.child0 != nil and u.child1 != nil
                assert(chunksize(u.child0) < chunksize(u.child1))
        u = u.fd
    dowhile u != t
    assert(head != nil)

//  Check all the chunks in a treebin.
fun do_check_treebin(my DlMalloc, i U64)
    let tb = my.treebin_at(i)
    let t = tb.*
    let empty = (my.treemap & (1U64 << i)) == 0
    if t == nil
        assert(empty)
    if not empty
        my.do_check_tree(t)

//  Check all the chunks in a smallbin.
fun do_check_smallbin(my DlMalloc, i U64)
    let b = my.smallbin_at(i)
    let p = b.bk
    let empty = (my.smallmap & (1U64 << i)) == 0
    if p == b
        assert(empty)
    if not empty
        while p != b
            let size = chunksize(p)
            let q *mchunk
            // each chunk claims to be free
            my.do_check_free_chunk(p)
            // chunk belongs in bin
            assert(small_index(size) == i)
            assert(p.bk == b or chunksize(p.bk) == chunksize(p))
            // chunk is followed by an inuse chunk
            q = next_chunk(p)
            if q.head != FENCEPOST_HEAD
                my.do_check_inuse_chunk(q)
            p = p.bk

// Find x in a bin. Used in other check functions.
fun bin_find(my DlMalloc, x *mchunk) Bool
    let size = chunksize(x)
    if is_small(size)
        let sidx = small_index(size)
        let b = my.smallbin_at(sidx)
        if my.smallmap_is_marked(sidx)
            let p = b
            do
                if p == x
                    return true
                p = p.fd
            dowhile p != b
    else
        let tidx = compute_tree_index(size)
        if my.treemap_is_marked(tidx)
            let t = my.treebin_at(tidx).*
            let sizebits = size << leftshift_for_tree_index(tidx)
            while t != nil and chunksize(t) != size
                t = ife sizebits >> (SIZE_T_BITSIZE - SIZE_T_ONE) & 1 == 0 : t.child0 : t.child1
                sizebits <<= 1
            if t != nil
                let u = t
                do
                    if u == castPointer<*tchunk>(x)
                        return true
                    u = u.fd
                dowhile u != t
    return false

// Check all properties of malloc_state.
[pub] fun CheckHeap(my DlMalloc) HeapStats
    assert(my.is_initialized())

    // check bins
    for i in 0..NSMALLBINS
        my.do_check_smallbin(i)
    for i in 0..NTREEBINS
        my.do_check_treebin(i)

    if my.dvsize != 0
        // check dv chunk
        my.do_check_any_chunk(my.dv)
        assert(my.dvsize == chunksize(my.dv))
        assert(my.dvsize >= MIN_CHUNK_SIZE)
        assert(not my.bin_find(my.dv))

    if my.top != nil
        // check top chunk
        my.do_check_top_chunk(my.top)
        assert(my.topsize > 0)
        assert(not my.bin_find(my.top))

    let usedChunks = 0
    let freeChunks = 0
    let freeBytes U64 = 0
    let usedBytes U64 = 0
    let heapSize U64 = 0
    heapSize += my.topsize + TOP_FOOT_SIZE

    // Walk segments
    let s = my.seg
    while s != nil
        let q = align_as_chunk(s.baseAddr)
        let lastq *mchunk = nil
        assert(pinuse(q))

        // Walk chunks
        while (my.segment_holds(s, q)
                and q != my.top and q.head != FENCEPOST_HEAD)
            let chunkSize = chunksize(q)
            heapSize += chunkSize
            if is_inuse(q)
                usedBytes += chunkSize
                usedChunks += 1
                assert(not my.bin_find(q))
                my.do_check_inuse_chunk(q)
            else
                freeBytes += chunkSize
                freeChunks += 1
                assert(q == my.dv or my.bin_find(q))
                assert(lastq == nil or is_inuse(lastq)) // Not 2 consecutive free
                my.do_check_free_chunk(q)
            lastq = q
            q = next_chunk(q)
        s = s.next


    assert(heapSize <= my.footprint)
    assert(my.footprint <= my.max_footprint)

    let stats = HeapStats()
    stats.HeapSize = heapSize.toIntMask
    stats.FreeChunks = freeChunks
    stats.FreeBytes = freeBytes.toIntMask
    stats.UsedChunks = usedChunks
    stats.UsedBytes = usedBytes.toIntMask
    return stats

    // ----------------------- Operations on smallbins -----------------------

    //  Various forms of linking and unlinking are defined as macros.  Even
    //  the ones for trees, which are very long but have very short typical
    //  paths.  This is ugly but reduces reliance on inlining support of
    //  compilers.

// Link a free chunk into a smallbin
[MethodImpl(MethodImplOptions.AggressiveInlining)]
fun insert_small_chunk(my mut DlMalloc, p *mchunk, s U64)
    let I = small_index(s)
    let B = my.smallbin_at(I)
    let F = B
    assert(s >= MIN_CHUNK_SIZE)
    if not my.smallmap_is_marked(I)
        my.mark_smallmap(I)
    elif RTCHECK(my.ok_address(B.fd))
        F = B.fd
    else
        my.CallCorruptionErrorAction()
    B.fd = p
    F.bk = p
    p.fd = F
    p.bk = B

// Unlink a chunk from a smallbin
[MethodImpl(MethodImplOptions.AggressiveInlining)]
fun unlink_small_chunk(my mut DlMalloc, p *mchunk, s U64)
    let F = p.fd
    let B = p.bk
    let I = small_index(s)
    assert(p != B)
    assert(p != F)
    assert(chunksize(p) == small_index2size(I))
    if RTCHECK(F == my.smallbin_at(I) or (my.ok_address(F) and F.bk == p))
        if B == F
            my.clear_smallmap(I)
        elif RTCHECK(B == my.smallbin_at(I)
                or (my.ok_address(B) and B.fd == p))
            F.bk = B
            B.fd = F
        else
            my.CallCorruptionErrorAction()
    else
        my.CallCorruptionErrorAction()

// Unlink the first chunk from a smallbin
[MethodImpl(MethodImplOptions.AggressiveInlining)]
fun unlink_first_small_chunk(my mut DlMalloc, b *mchunk, p *mchunk, i U64)
    let F = p.fd
    assert(p != b)
    assert(p != F)
    assert(chunksize(p) == small_index2size(i))
    if b == F
        my.clear_smallmap(i)
    elif RTCHECK(my.ok_address(F) and F.bk == p)
        F.bk = b
        b.fd = F
    else
        my.CallCorruptionErrorAction()

// Replace dv node, binning the old one
// Used only when dvsize known to be small
[MethodImpl(MethodImplOptions.AggressiveInlining)]
fun replace_dv(my mut DlMalloc, p *mchunk, s U64)
    let DVS = my.dvsize
    assert(is_small(DVS))
    if DVS != 0
        let DV = my.dv
        my.insert_small_chunk(DV, DVS)
    my.dvsize = s
    my.dv = p

// Insert chunk into tree
fun insert_large_chunk(my mut DlMalloc, x *tchunk, s U64)
    let I = compute_tree_index(s)
    let H = my.treebin_at(I)
    x.index = I
    x.child0 = nil
    x.child1 = nil
    if not my.treemap_is_marked(I)
        my.mark_treemap(I)
        H.* = x
        x.parent = castPointer<*tchunk>(H)
        x.fd = x
        x.bk = x
    else
        let T = H.*
        let K = s << leftshift_for_tree_index(I)
        while true
            if chunksize(T) != s
                let C = ife (K >> (SIZE_T_BITSIZE - SIZE_T_ONE)) & 1 == 0 : &*T.child0 : &*T.child1
                K <<= 1
                if C.* != nil
                    T = C.*
                elif RTCHECK(my.ok_address(C))
                    C.* = x
                    x.parent = T
                    x.fd = x
                    x.bk = x
                    break
                else
                    my.CallCorruptionErrorAction()
                    break
            else
                let F = T.fd
                if RTCHECK(my.ok_address(T) and my.ok_address(F))
                    T.fd = x
                    F.bk = x
                    x.fd = F
                    x.bk = T
                    x.parent = nil
                    break
                else
                    my.CallCorruptionErrorAction()
                    break

//  Unlink steps:
//
//  1. If x is a chained node, unlink it from its same-sized fd/bk links
//     and choose its bk node as its replacement.
//  2. If x was the last node of its size, but not a leaf node, it must
//     be replaced with a leaf node (not merely one with an open left or
//     right), to make sure that lefts and rights of descendents
//     correspond properly to bit masks.  We use the rightmost descendent
//     of x.  We could use any other leaf, but this is easy to locate and
//     tends to counteract removal of leftmosts elsewhere, and so keeps
//     paths shorter than minimally guaranteed.  This doesn't loop much
//     because on average a node in a tree is near the bottom.
//  3. If x is the base of a chain (i.e., has parent links) relink
//     x's parent and children to x's replacement (or nil if none).
fun unlink_large_chunk(my mut DlMalloc, x *tchunk)
    let XP = x.parent
    let R *tchunk
    if x.bk != x
        let F = x.fd
        R = x.bk
        if RTCHECK(my.ok_address(F) and F.bk == x and R.fd == x)
            F.bk = R
            R.fd = F
        else
            my.CallCorruptionErrorAction()
    else
        let RP = &*x.child1
        R = RP.*
        if R == nil
            RP = &*x.child0
            R = RP.*
        if R != nil
            let CP = &*R.child1
            if CP.* == nil
                CP = &*R.child0
            while CP.* != nil
                RP = CP
                R = RP.*
                CP = &*R.child1
                if CP.* == nil
                    CP = &*R.child0
            if RTCHECK(my.ok_address(RP))
                RP.* = nil
            else
                my.CallCorruptionErrorAction()
    if XP != nil
        let H = my.treebin_at(x.index)
        if x == H.*
            H.* = R
            if R == nil
                my.clear_treemap(x.index)
        elif RTCHECK(my.ok_address(XP))
            if (XP.child0 == x)
                XP.child0 = R
            else
                XP.child1 = R
        else
            my.CallCorruptionErrorAction()
        if R != nil
            if RTCHECK(my.ok_address(R))
                R.parent = XP
                let C0 = x.child0
                if C0 != nil
                    if RTCHECK(my.ok_address(C0))
                        R.child0 = C0
                        C0.parent = R
                    else
                        my.CallCorruptionErrorAction()
                let C1 = x.child1
                if C1 != nil
                    if RTCHECK(my.ok_address(C1))
                        R.child1 = C1
                        C1.parent = R
                    else
                        my.CallCorruptionErrorAction()
            else
                my.CallCorruptionErrorAction()

[MethodImpl(MethodImplOptions.AggressiveInlining)]
fun insert_chunk(my mut DlMalloc, p *mchunk, s U64)
    if is_small(s)
        my.insert_small_chunk(p, s)
    else
        my.insert_large_chunk(castPointer<*tchunk>(p), s)

[MethodImpl(MethodImplOptions.AggressiveInlining)]
fun unlink_chunk(my mut DlMalloc, p *mchunk, s U64)
    if is_small(s)
        my.unlink_small_chunk(p, s)
    else
        my.unlink_large_chunk(castPointer<*tchunk>(p))

// Initialize top chunk and its size
fun init_top(my mut DlMalloc, p *mchunk, psize U64)
    // Ensure alignment
    let offset = align_offset(chunk2mem(p))
    p = castPointer<*mchunk>(castPointer<*Byte>(p) + offset)
    psize -= offset

    my.top = p
    my.topsize = psize
    p.head = psize | PINUSE_BIT
    // set size of fake trailing chunk holding overhead space only once
    chunk_plus_offset(p, psize).head = TOP_FOOT_SIZE
    my.trim_check = my.mparams.trim_threshold // reset on each update

// Initialize bins for a new mstate that is otherwise zeroed out
fun init_bins(my mut DlMalloc)
    // Establish circular links for smallbins
    for i in 0..NSMALLBINS
        let bin = my.smallbin_at(i)
        bin.bk = bin
        bin.fd = bin

// Default corruption action - forget all allocated memory.
fun ResetOnError(my mut DlMalloc)
    my.malloc_corruption_error_count += 1
    // Reinitialize fields to forget about all memory
    my.smallmap = 0
    my.treemap = 0
    my.dvsize = 0
    my.topsize = 0
    my.seg.baseAddr = nil
    my.seg.size = 0
    my.seg.next = nil
    my.top = nil
    my.dv = nil
    for i in 0..NTREEBINS
        my.treebin_at(i).* = nil
    my.init_bins()

// Allocate chunk and prepend remainder with chunk in successor base.
fun prepend_alloc(my mut DlMalloc, newbase *Byte, oldbase *Byte, nb U64) *Void
    let p = align_as_chunk(newbase)
    let oldfirst = align_as_chunk(oldbase)
    let psize = castPointer<U64>(castPointer<*Byte>(oldfirst) - castPointer<*Byte>(p))
    let q = chunk_plus_offset(p, nb)
    let qsize = psize - nb
    set_size_and_pinuse_of_inuse_chunk(p, nb)

    assert(castPointer<*Byte>(oldfirst) > castPointer<*Byte>(q))
    assert(pinuse(oldfirst))
    assert(qsize >= MIN_CHUNK_SIZE)

    // consolidate remainder with first chunk of old base
    if oldfirst == my.top
        my.topsize += qsize
        let tsize = my.topsize
        my.top = q
        q.head = tsize | PINUSE_BIT
        my.check_top_chunk(q)
    elif oldfirst == my.dv
        my.dvsize += qsize
        let dsize = my.dvsize
        my.dv = q
        set_size_and_pinuse_of_free_chunk(q, dsize)
    else
        if not is_inuse(oldfirst)
            let nsize = chunksize(oldfirst)
            my.unlink_chunk(oldfirst, nsize)
            oldfirst = chunk_plus_offset(oldfirst, nsize)
            qsize += nsize
        set_free_with_pinuse(q, qsize, oldfirst)
        my.insert_chunk(q, qsize)
        my.check_free_chunk(q)

    my.check_malloced_chunk(chunk2mem(p), nb)
    return chunk2mem(p)

// Add a segment to hold a new noncontiguous region
fun add_segment(my mut DlMalloc, tbase *Byte, tsize U64)
    // Determine locations and sizes of segment, fenceposts, old top
    let old_top = castPointer<*Byte>(my.top)
    let oldsp = my.segment_holding(old_top)
    let old_end = oldsp.baseAddr + oldsp.size
    let ssize = pad_request(sizeof(msegment).toU64Mask)
    let rawsp = old_end - (ssize + FOUR_SIZE_T_SIZES + CHUNK_ALIGN_MASK)
    let offset = align_offset(chunk2mem(rawsp))
    let asp = rawsp + offset
    let csp = ife asp < old_top + MIN_CHUNK_SIZE : old_top : asp
    let sp = castPointer<*mchunk>(csp)
    let ss = castPointer<*msegment>(chunk2mem(sp))
    let tnext = chunk_plus_offset(sp, ssize)
    let p = tnext
    let nfences = 0

    // reset top to new space
    my.init_top(castPointer<*mchunk>(tbase), tsize - TOP_FOOT_SIZE)

    // Set up segment record
    assert(is_aligned(ss))
    set_size_and_pinuse_of_inuse_chunk(sp, ssize)
    ss.* = my.seg.* // Push current record
    my.seg.baseAddr = tbase
    my.seg.size = tsize
    my.seg.next = ss

    // Insert trailing fenceposts
    while true
        let nextp = chunk_plus_offset(p, SIZE_T_SIZE)
        p.head = FENCEPOST_HEAD
        nfences += 1
        if castPointer<*Byte>(&*nextp.head_ptr) < old_end
            p = nextp
        else
            break
    assert(nfences >= 2)

    // Insert the rest of old top into a bin as an ordinary free chunk
    if csp != old_top
        let q = castPointer<*mchunk>(old_top)
        let psize = castPointer<U64>(csp - old_top)
        let tn = chunk_plus_offset(q, psize)
        set_free_with_pinuse(q, psize, tn)
        my.insert_chunk(q, psize)

    my.check_top_chunk(my.top)

// Get memory from system using MMAP
fun sys_alloc(my mut DlMalloc, nb U64) *Void
    let tbase *Byte = nil
    let tsize = 0U64
    let asize U64 // allocation size

    my.ensure_initialization()

    asize = my.granularity_align(nb + SYS_ALLOC_PADDING)
    if asize <= nb
        return nil // wraparound

    // Try getting memory via CallMoreCore
    // In all cases, we need to request enough bytes from system to ensure
    // we can malloc nb bytes upon success, so pad with enough space for
    // top_foot, plus alignment-pad to make sure we don't lose bytes if
    // not on boundary, and round this up to a granularity unit.

    if tbase == nil
        // Try MMAP
        let mp = castPointer<*Byte>(my.CallMoreCore(&asize))
        if mp != nil
            tbase = mp
            tsize = asize

    if tbase != nil
        my.footprint += tsize
        if my.footprint > my.max_footprint
            my.max_footprint = my.footprint

        if not my.is_initialized()
            panicFatal("Must already be initialized")

        // Try to merge with an existing segment
        // Only consider most recent segment if traversal suppressed
        let sp = my.seg
        while sp != nil and tbase != sp.baseAddr + sp.size
            sp = ife NO_SEGMENT_TRAVERSAL : nil : sp.next

        if sp != nil
                and my.segment_holds(sp, my.top)
            // append
            sp.size += tsize
            my.init_top(my.top, my.topsize + tsize)
        else
            if tbase < my.least_addr
                my.least_addr = tbase
            sp = my.seg
            while sp != nil and sp.baseAddr != tbase + tsize
                sp = ife NO_SEGMENT_TRAVERSAL : nil : sp.next
            if sp != nil
                let oldbase = sp.baseAddr
                sp.baseAddr = tbase
                sp.size += tsize
                return my.prepend_alloc(tbase, oldbase, nb)
            else
                my.add_segment(tbase, tsize)

        if nb < my.topsize
            // Allocate from new or extended top space
            my.topsize -= nb
            let rsize = my.topsize
            let p = my.top
            my.top = chunk_plus_offset(p, nb)
            let r = my.top
            r.head = rsize | PINUSE_BIT
            set_size_and_pinuse_of_inuse_chunk(p, nb)
            my.check_top_chunk(my.top)
            my.check_malloced_chunk(chunk2mem(p), nb)
            return chunk2mem(p)

    my.CallMallocFailureAction()
    return nil


// Unmap and unlink any mmapped segments that don't contain used chunks
fun release_unused_segments(my mut DlMalloc) U64
    let released U64 = 0
    let nsegs U64 = 0
    let pred = my.seg
    let sp = pred.next
    while sp != nil
        let baseAddr = sp.baseAddr
        let size = sp.size
        let next = sp.next
        nsegs += 1
        let p = align_as_chunk(baseAddr)
        let psize = chunksize(p)
        // Can unmap if first chunk holds entire segment and not pinned
        if not is_inuse(p) and castPointer<*Byte>(p) + psize >= baseAddr + size - TOP_FOOT_SIZE
            let tp = castPointer<*tchunk>(p)
            assert(my.segment_holds(sp, castPointer<*Byte>(sp)))
            if p == my.dv
                my.dv = nil
                my.dvsize = 0
            else
                my.unlink_large_chunk(tp)
            if my.CallReleaseCore(baseAddr, size)
                released += size
                my.footprint -= size
                // unlink obsoleted record
                sp = pred
                sp.next = next
            else
                // back out if cannot unmap
                my.insert_large_chunk(tp, psize)
        if NO_SEGMENT_TRAVERSAL  // scan only first segment
            break
        pred = sp
        sp = next
    // Reset check counter
    my.release_checks = ife nsegs > MAX_RELEASE_CHECK_RATE : nsegs : MAX_RELEASE_CHECK_RATE
    return released

fun sys_trim(my mut DlMalloc, pad U64) Int
    let released U64 = 0
    my.ensure_initialization()
    if pad < MAX_REQUEST and my.is_initialized()
        pad += TOP_FOOT_SIZE // ensure enough room for segment overhead

        if my.topsize > pad
            // Shrink top space in granularity-size units, keeping at least one
            let unit = my.mparams.granularity
            let extra = ((my.topsize - pad + (unit - SIZE_T_ONE)) / unit
                            - SIZE_T_ONE) * unit
            let sp = my.segment_holding(castPointer<*Byte>(my.top))

            if sp.size >= extra
                    and not my.has_segment_link(sp)
                // can't shrink if pinned
                let newsize = sp.size - extra
                if my.CallReleaseCore(sp.baseAddr + newsize, extra)
                    released = extra

            if released != 0
                sp.size -= released
                my.footprint -= released
                my.init_top(my.top, my.topsize - released)
                my.check_top_chunk(my.top)

        // Unmap any unused mmapped segments
        released += my.release_unused_segments()

        // On failure, disable autotrim to avoid repeated failed future calls
        if released == 0 and my.topsize > my.trim_check
            my.trim_check = MAX_SIZE_T

    return ife released != 0 : 1 : 0

// Consolidate and bin a chunk. Differs from exported versions
//   of free mainly in that the chunk need not be marked as inuse.
fun dispose_chunk(my mut DlMalloc, p *mchunk, psize U64)
    let next = chunk_plus_offset(p, psize)
    if not pinuse(p)
        let prev *mchunk
        let prevsize = p.prev_foot
        prev = chunk_minus_offset(p, prevsize)
        psize += prevsize
        p = prev
        if RTCHECK(my.ok_address(prev))
            // consolidate backward
            if p != my.dv
                my.unlink_chunk(p, prevsize)
            elif (next.head & INUSE_BITS) == INUSE_BITS
                my.dvsize = psize
                set_free_with_pinuse(p, psize, next)
                return
        else
            my.CallCorruptionErrorAction()
            return
    if RTCHECK(my.ok_address(next))
        if not cinuse(next)
            // consolidate forward
            if next == my.top
                my.topsize += psize
                let tsize = my.topsize
                my.top = p
                p.head = tsize | PINUSE_BIT
                if p == my.dv
                    my.dv = nil
                    my.dvsize = 0
                return
            elif next == my.dv
                my.dvsize += psize
                let dsize = my.dvsize
                my.dv = p
                set_size_and_pinuse_of_free_chunk(p, dsize)
                return
            else
                let nsize = chunksize(next)
                psize += nsize
                my.unlink_chunk(next, nsize)
                set_size_and_pinuse_of_free_chunk(p, psize)
                if p == my.dv
                    my.dvsize = psize
                    return
        else
            set_free_with_pinuse(p, psize, next)
        my.insert_chunk(p, psize)
    else
        my.CallCorruptionErrorAction()


// allocate a large request from the best fitting chunk in a treebin
fun tmalloc_large(my mut DlMalloc, nb U64) *Void
    let v *tchunk = nil
    let rsize = -nb // Unsigned negation
    let t *tchunk
    let idx = compute_tree_index(nb)
    t = my.treebin_at(idx).*
    if t != nil
        // Traverse tree for this bin looking for node with size == nb
        let sizebits = nb << leftshift_for_tree_index(idx)
        let rst *tchunk = nil  // The deepest untaken right subtree
        while true
            let rt *tchunk
            let trem = chunksize(t) - nb
            if trem < rsize
                v = t
                rsize = trem
                if rsize == 0
                    break
            rt = t.child1
            t = ife (sizebits >> (SIZE_T_BITSIZE - SIZE_T_ONE)) & 1 == 0 : t.child0 : t.child1
            if rt != nil and rt != t
                rst = rt
            if t == nil
                t = rst // set t to least subtree holding sizes > nb
                break
            sizebits <<= 1
    if t == nil and v == nil
        // set t to root of next non-empty treebin
        let leftbits = left_bits(idx2bit(idx)) & my.treemap
        if leftbits != 0
            let leastbit = least_bit(leftbits)
            let i = compute_bit2idx(leastbit)
            t = my.treebin_at(i).*

    while t != nil
        // find smallest of tree or subtree
        let trem2 = chunksize(t) - nb
        if trem2 < rsize
            rsize = trem2
            v = t
        t = leftmost_child(t)

    // If dv is a better fit, return 0 so malloc will use it
    if v != nil and rsize < (my.dvsize - nb)
        if RTCHECK(my.ok_address(v))
            // split
            let r = chunk_plus_offset(v, nb)
            assert(chunksize(v) == rsize + nb)
            if RTCHECK(ok_next(v, r))
                my.unlink_large_chunk(v)
                if rsize < MIN_CHUNK_SIZE
                    set_inuse_and_pinuse(v, (rsize + nb))
                else
                    set_size_and_pinuse_of_inuse_chunk(castPointer<*mchunk>(v), nb)
                    set_size_and_pinuse_of_free_chunk(r, rsize)
                    my.insert_chunk(r, rsize)
                return chunk2mem(v)
        my.CallCorruptionErrorAction()
    return nil

// allocate a small request from the best fitting chunk in a treebin
fun tmalloc_small(my mut DlMalloc, nb U64) *Void
    let t *tchunk
    let v *tchunk
    let rsize U64
    let leastbit = least_bit(my.treemap)
    let i = compute_bit2idx(leastbit)
    t = my.treebin_at(i).*
    v = t
    rsize = chunksize(t) - nb

    t = leftmost_child(t)
    while t != nil
        let trem = chunksize(t) - nb
        if trem < rsize
            rsize = trem
            v = t
        t = leftmost_child(t)

    if RTCHECK(my.ok_address(v))
        let r = chunk_plus_offset(v, nb)
        assert(chunksize(v) == rsize + nb)
        if RTCHECK(ok_next(v, r))
            my.unlink_large_chunk(v)
            if rsize < MIN_CHUNK_SIZE
                set_inuse_and_pinuse(v, (rsize + nb))
            else
                set_size_and_pinuse_of_inuse_chunk(castPointer<*mchunk>(v), nb)
                set_size_and_pinuse_of_free_chunk(r, rsize)
                my.replace_dv(r, rsize)
            return chunk2mem(v)

    my.CallCorruptionErrorAction()
    return nil

// Traversal
fun internal_inspect_all(my DlMalloc, handler fun(start *Void, next *Void, used U64))
    if not my.is_initialized()
        return

    let top2 = my.top
    let s = my.seg
    while s != nil
        let q = align_as_chunk(s.baseAddr)
        while my.segment_holds(s, q) and q.head != FENCEPOST_HEAD
            let next = next_chunk(q)
            let sz = chunksize(q)
            let used U64
            let start *Void
            if is_inuse(q)
                used = sz - CHUNK_OVERHEAD // must not be mmapped
                start = chunk2mem(q)
            else
                used = 0
                if is_small(sz)
                    // offset by possible bookkeeping
                    start = castPointer<*Void>(castPointer<*Byte>(q) + sizeof(mchunk))
                else
                    start = castPointer<*Void>(castPointer<*Byte>(q) + sizeof(tchunk))
            if start < castPointer<*Void>(next)  // skip if all space is bookkeeping
                handler(start, next, used)
            if q == top2 
                break
            q = next
        s = s.next

// TBD: Optimize and move to standard library
fun memClear(mem *Void, size Int)
    let b = castPointer<*Byte>(mem)
    for s in 0..size
        b.* = 0Byte
        b = b+1 // TBD: Allow b+=1


fun init_user_mstate(my mut DlMalloc, tbase *Byte, tsize U64)
    let msp = align_as_chunk(tbase)
    let mem *Void = chunk2mem(msp)

    my.smallbins = castPointer<**mchunk>(AllocHGlobal(sizeof(*mchunk).toU64Mask * ((NSMALLBINS + 1) * 2)))
    my.treebins = castPointer<**tchunk>(AllocHGlobal(sizeof(*tchunk).toU64Mask * NTREEBINS))
    my.seg = castPointer<*msegment>(AllocHGlobal(sizeof(msegment).toU64Mask))
    memClear(my.smallbins, sizeof(*mchunk) * (((NSMALLBINS + 1) * 2)).toIntMask)
    memClear(my.treebins, sizeof(*tchunk) * NTREEBINS.toIntMask)
    memClear(my.seg, sizeof(msegment))

    let msize U64 = 0
    msp.head = (msize | INUSE_BITS)
    my.seg.baseAddr = tbase
    my.least_addr = tbase
    my.seg.size = tsize
    my.footprint = tsize
    my.max_footprint = tsize
    my.magic = my.mparams.magic
    my.release_checks = MAX_RELEASE_CHECK_RATE

    my.init_bins()
    let mn = next_chunk(mem2chunk(mem))
    my.init_top(mn, castPointer<U64>((tbase + tsize) - castPointer<*Byte>(mn)) - TOP_FOOT_SIZE)
    my.check_top_chunk(my.top)


// Allocate a block of memory
[pub] fun malloc(my mut DlMalloc, length U64) *Void
    //     Basic algorithm:
    //     If a small request (< 256 bytes minus per-chunk overhead):
    //       1. If one exists, use a remainderless chunk in associated smallbin.
    //          (Remainderless means that there are too few excess bytes to
    //          represent as a chunk.)
    //       2. If it is big enough, use the dv chunk, which is normally the
    //          chunk adjacent to the one used for the most recent small request.
    //       3. If one exists, split the smallest available chunk in a bin,
    //          saving remainder in dv.
    //       4. If it is big enough, use the top chunk.
    //       5. If available, get memory from system and use it
    //     Otherwise, for a large request:
    //       1. Find the smallest available binned chunk that fits, and use it
    //          if it is better fitting than dv chunk, splitting if necessary.
    //       2. If better fitting than any binned chunk, use the dv chunk.
    //       3. If it is big enough, use the top chunk.
    //       4. If request size >= mmap threshold, try to directly mmap this chunk.
    //       5. If available, get memory from system and use it

    if not my.ok_magic()
        my.CallUsageErrorAction(nil)
        return nil
    let mem *Void
    let nb U64
    if length <= MAX_SMALL_REQUEST
        let idx U64
        let smallbits U64
        nb = ife length < MIN_REQUEST : MIN_CHUNK_SIZE : pad_request(length)
        idx = small_index(nb)
        smallbits = my.smallmap >> idx

        if (smallbits & 0x3U64) != 0
            // Remainderless fit to a smallbin.
            let b *mchunk
            let p *mchunk
            idx += ~smallbits & 1       // Uses next bin if idx empty
            b = my.smallbin_at(idx)
            p = b.fd
            assert(chunksize(p) == small_index2size(idx))
            my.unlink_first_small_chunk(b, p, idx)
            set_inuse_and_pinuse(p, small_index2size(idx))
            mem = chunk2mem(p)
            my.check_malloced_chunk(mem, nb)
            return mem
        elif nb > my.dvsize
            if smallbits != 0
                // Use chunk in next nonempty smallbin
                let b *mchunk
                let p *mchunk
                let r *mchunk
                let rsize U64
                let leftbits = (smallbits << idx) & left_bits(idx2bit(idx))
                let leastbit = least_bit(leftbits)
                let i = compute_bit2idx(leastbit)
                b = my.smallbin_at(i)
                p = b.fd
                assert(chunksize(p) == small_index2size(i))
                my.unlink_first_small_chunk(b, p, i)
                rsize = small_index2size(i) - nb
                // Fit here cannot be remainderless if 4byte sizes
                if SIZE_T_SIZE != 4 and rsize < MIN_CHUNK_SIZE
                    set_inuse_and_pinuse(p, small_index2size(i))
                else
                    set_size_and_pinuse_of_inuse_chunk(p, nb)
                    r = chunk_plus_offset(p, nb)
                    set_size_and_pinuse_of_free_chunk(r, rsize)
                    my.replace_dv(r, rsize)
                mem = chunk2mem(p)
                my.check_malloced_chunk(mem, nb)
                return mem
            else
                if my.treemap != 0
                    mem = my.tmalloc_small(nb)
                    if mem != nil
                        my.check_malloced_chunk(mem, nb)
                        return mem
    elif length >= MAX_REQUEST
        nb = MAX_SIZE_T// Too big to allocate. Force failure (in sys alloc)
    else
        nb = pad_request(length)
        if my.treemap != 0
            mem = my.tmalloc_large(nb)
            if mem != nil
                my.check_malloced_chunk(mem, nb)
                return mem

    if nb <= my.dvsize
        let rsize = my.dvsize - nb
        let p = my.dv
        if rsize >= MIN_CHUNK_SIZE
            // split dv
            my.dv = chunk_plus_offset(p, nb)
            let r = my.dv
            my.dvsize = rsize
            set_size_and_pinuse_of_free_chunk(r, rsize)
            set_size_and_pinuse_of_inuse_chunk(p, nb)
        else
            // exhaust dv
            let dvs = my.dvsize
            my.dvsize = 0
            my.dv = nil
            set_inuse_and_pinuse(p, dvs)
        mem = chunk2mem(p)
        my.check_malloced_chunk(mem, nb)
        return mem
    elif nb < my.topsize
        // Split top
        my.topsize -= nb
        let rsize = my.topsize
        let p = my.top
        my.top = chunk_plus_offset(p, nb)
        let r = my.top
        r.head = rsize | PINUSE_BIT
        set_size_and_pinuse_of_inuse_chunk(p, nb)
        mem = chunk2mem(p)
        my.check_top_chunk(my.top)
        my.check_malloced_chunk(mem, nb)
        return mem

    mem = my.sys_alloc(nb)
    return mem

// Free a block of memory
[pub] fun free(my mut DlMalloc, mem *Void)
    // Consolidate freed chunks with preceeding or succeeding bordering
    // free chunks, if they exist, and then place in a bin.  Intermixed
    // with special cases for top, dv, mmapped chunks, and usage errors.

    if mem == nil
        return

    let p = mem2chunk(mem)
    if not my.ok_magic()
        my.CallUsageErrorAction(p)
        return
    my.check_inuse_chunk(p)
    if (not RTCHECK(my.ok_address(p) and ok_inuse(p)))
        my.CallUsageErrorAction(p)
        return
    assert(not is_mmapped(p))

    let psize = chunksize(p)
    let next = chunk_plus_offset(p, psize)
    if not pinuse(p)
        let prevsize = p.prev_foot
        let prev = chunk_minus_offset(p, prevsize)
        psize += prevsize
        p = prev
        if RTCHECK(my.ok_address(prev))
            // consolidate backward
            if p != my.dv
                my.unlink_chunk(p, prevsize)
            elif (next.head & INUSE_BITS) == INUSE_BITS
                my.dvsize = psize
                set_free_with_pinuse(p, psize, next)
                return
        else
            my.CallUsageErrorAction(p)
            return

    if RTCHECK(ok_next(p, next) and ok_pinuse(next))
        if not cinuse(next)
            // consolidate forward
            if next == my.top
                my.topsize += psize
                let tsize = my.topsize
                my.top = p
                p.head = tsize | PINUSE_BIT
                if p == my.dv
                    my.dv = nil
                    my.dvsize = 0
                if my.should_trim(tsize)
                    my.sys_trim(0U64)
                return
            elif next == my.dv
                my.dvsize += psize
                let dsize = my.dvsize
                my.dv = p
                set_size_and_pinuse_of_free_chunk(p, dsize)
                return
            else
                let nsize = chunksize(next)
                psize += nsize
                my.unlink_chunk(next, nsize)
                set_size_and_pinuse_of_free_chunk(p, psize)
                if p == my.dv
                    my.dvsize = psize
                    return
        else
            set_free_with_pinuse(p, psize, next)

        if is_small(psize)
            my.insert_small_chunk(p, psize)
            my.check_free_chunk(p)
        else
            let tp = castPointer<*tchunk>(p)
            my.insert_large_chunk(tp, psize)
            my.check_free_chunk(p)
            my.release_checks -= 1
            if my.release_checks == 0
                my.release_unused_segments()
        return
    my.CallUsageErrorAction(p)


[pub] fun inspectAll(my DlMalloc, handler fun(start *Void, next *Void, used U64))
    if my.ok_magic()
        my.internal_inspect_all(handler)
    else
        my.CallUsageErrorAction(nil)

[pub] fun trim(my mut DlMalloc, pad U64) Int
    let result = 0
    if my.ok_magic()
        result = my.sys_trim(pad)
    else
        my.CallUsageErrorAction(nil)
    return result

// ------ End of DlMalloc ------

[pub] type HeapStats
    HeapSize Int
    UsedChunks Int
    UsedBytes Int
    FreeChunks Int
    FreeBytes Int

[pub] fun toStr(my HeapStats) Str
    return "HeapSize={h.HeapSize}"
            ", UB={h.UsedBytes}"
            ", FB={h.FreeBytes}"
            ", UC={h.UsedChunks}"
            ", FC={h.FreeChunks}"


// -----------------------  Chunk representations ------------------------

//  (The following includes lightly edited explanations by Colin Plumb.)
//
//  The malloc_chunk declaration below is misleading (but accurate and
//  necessary).  It declares a "view" into memory allowing access to
//  necessary fields at known offsets from a given base.
//
//  Chunks of memory are maintained using a `boundary tag' method as
//  originally described by Knuth.  (See the paper by Paul Wilson
//  ftp://ftp.cs.utexas.edu/[pub]/garbage/allocsrv.ps for a survey of such
//  techniques.)  Sizes of free chunks are stored both in the front of
//  each chunk and at the end.  This makes consolidating fragmented
//  chunks into bigger chunks fast.  The head fields also hold bits
//  representing whether chunks are free or in use.
//
//  Here are some pictures to make it clearer.  They are "exploded" to
//  show that the state of a chunk can be thought of as extending from
//  the high 31 bits of the head field of its header through the
//  prev_foot and PINUSE_BIT bit of the following chunk header.
//
//  A chunk that's in use looks like:
//
//   chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//           | Size of previous chunk (if P = 0)                             |
//           +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |P|
//         | Size of this chunk                                         1| +-+
//   mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//         |                                                               |
//         +-                                                             -+
//         |                                                               |
//         +-                                                             -+
//         |                                                               :
//         +-      size - sizeof(size_t) available payload bytes          -+
//         :                                                               |
// chunk-> +-                                                             -+
//         |                                                               |
//         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |1|
//       | Size of next chunk (may or may not be in use)               | +-+
// mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

//    And if it's free, it looks like this:

//   chunk-> +-                                                             -+
//           | User payload (must be in use, or we would have merged!)       |
//           +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |P|
//         | Size of this chunk                                         0| +-+
//   mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//         | Next pointer                                                  |
//         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//         | Prev pointer                                                  |
//         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//         |                                                               :
//         +-      size - sizeof(struct chunk) unused bytes               -+
//         :                                                               |
// chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//         | Size of this chunk                                            |
//         +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |0|
//       | Size of next chunk (must be in use, or we would have merged)| +-+
// mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//       |                                                               :
//       +- User payload                                                -+
//       :                                                               |
//       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//                                                                     |0|
//                                                                     +-+
//  Note that since we always merge adjacent free chunks, the chunks
//  adjacent to a free chunk must be in use.
//
//  Given a pointer to a chunk (which can be derived trivially from the
//  payload pointer) we can, in O(1) time, find out whether the adjacent
//  chunks are free, and if so, unlink them from the lists that they
//  are on and merge them with the current chunk.
//
//  Chunks always begin on even word boundaries, so the mem portion
//  (which is returned to the user) is also on an even word boundary, and
//  thus at least double-word aligned.
//
//  The P (PINUSE_BIT) bit, stored in the unused low-order bit of the
//  chunk size (which is always a multiple of two words), is an in-use
//  bit for the *previous* chunk.  If that bit is *clear*, then the
//  word before the current chunk size contains the previous chunk
//  size, and can be used to find the front of the previous chunk.
//  The very first chunk allocated always has this bit set, preventing
//  access to non-existent (or non-owned) memory. If pinuse is set for
//  any given chunk, then you CANNOT determine the size of the
//  previous chunk, and might even get a memory addressing fault when
//  trying to do so.
//
//  The C (CINUSE_BIT) bit, stored in the unused second-lowest bit of
//  the chunk size redundantly records whether the current chunk is
//  inuse (unless the chunk is mmapped). This redundancy enables usage
//  checks within free and realloc, and reduces indirection when freeing
//  and consolidating chunks.
//
//  Each freshly allocated chunk must have both cinuse and pinuse set.
//  That is, each allocated chunk borders either a previously allocated
//  and still in-use chunk, or the base of its memory arena. This is
//  ensured by making all allocations from the `lowest' part of any
//  found chunk.  Further, no free chunk physically borders another one,
//  so each free chunk is known to be preceded and followed by either
//  inuse chunks or the ends of memory.
//
//  Note that the `foot' of the current chunk is actually represented
//  as the prev_foot of the NEXT chunk. This makes it easier to
//  deal with alignments etc but can be very confusing when trying
//  to extend or adapt this code.
//
//  The exceptions to all this are
//
//     1. The special chunk `top' is the top-most available chunk (i.e.,
//        the one bordering the end of available memory). It is treated
//        specially.  Top is never included in any bin, is used only if
//        no other chunk is available, and is released back to the
//        system if it is very large (see M_TRIM_THRESHOLD).  In effect,
//        the top chunk is treated as larger (and thus less well
//        fitting) than any other available chunk.  The top chunk
//        doesn't update its trailing size field since there is no next
//        contiguous chunk that would have to index off it. However,
//        space is still allocated for it (TOP_FOOT_SIZE) to enable
//        separation or merging when space is extended.
//
//     3. Chunks allocated via mmap, have both cinuse and pinuse bits
//        cleared in their head fields.  Because they are allocated
//        one-by-one, each must carry its own prev_foot field, which is
//        also used to hold the offset this chunk has within its mmapped
//        region, which is needed to preserve alignment. Each mmapped
//        chunk is trailed by the first two fields of a fake next-chunk
//        for sake of usage checks.
//
// Originally malloc_chunk, now mchunk
[pub] type mchunk
    prev_foot_ptr *Void // Size of previous chunk (if free).
    head_ptr *Void      // Size and inuse bits.
    fd *mchunk          // double links -- used only if free.
    bk *mchunk

fun get prev_foot(my mchunk) U64
    return castPointer<U64>(my.prev_foot_ptr)
fun set prev_foot(my mchunk, value U64)
    my.prev_foot_ptr = castPointer<*Void>(Xuint(value))
fun get head(my mchunk) U64
    return castPointer<U64>(my.head_ptr)
fun set head(my mchunk, value U64)
    my.head_ptr = castPointer<*Void>(Xuint(value))

fun cinuse(p *mchunk) Bool
    return (p.head & CINUSE_BIT) != 0
fun pinuse(p *mchunk) Bool
    return (p.head & PINUSE_BIT) != 0
fun flag4inuse(p *mchunk) Bool
    return (p.head & FLAG4_BIT) != 0
fun is_inuse(p *mchunk) Bool
    return (p.head & INUSE_BITS) != PINUSE_BIT
fun is_mmapped(p *mchunk) Bool
    return (p.head & INUSE_BITS) == 0
fun chunksize(p *mchunk) U64
    return p.head & ~FLAG_BITS
fun clear_pinuse(p *mchunk)
    p.head &= ~PINUSE_BIT
fun set_flag4(p *mchunk)
    p.head |= FLAG4_BIT
fun clear_flag4(p *mchunk)
    p.head &= ~FLAG4_BIT
fun chunk_plus_offset(p *mchunk, s U64) *mchunk
    return castPointer<*mchunk>(castPointer<*Byte>(p) + s)
fun chunk_minus_offset(p *mchunk, s U64) *mchunk
    return castPointer<*mchunk>(castPointer<*Byte>(p) - s)
fun next_chunk(p *mchunk) *mchunk
    return castPointer<*mchunk>(castPointer<*Byte>(p) + (p.head & ~FLAG_BITS))
fun prev_chunk(p *mchunk) *mchunk
    return castPointer<*mchunk>(castPointer<*Byte>(p) - (p.prev_foot))
fun next_pinuse(p *mchunk) Bool
    return ((next_chunk(p).head) & PINUSE_BIT) != 0
fun get_foot(p *mchunk, s U64) U64
    return ((castPointer<*mchunk>(castPointer<*Byte>(p) + s)).prev_foot)
fun set_foot(p *mchunk, s U64)
    (castPointer<*mchunk>(castPointer<*Byte>(p) + s)).prev_foot = s
fun set_size_and_pinuse_of_free_chunk(p *mchunk, s U64)
    p.head = s | PINUSE_BIT; set_foot(p, s)
fun set_free_with_pinuse(p *mchunk, s U64, n *mchunk)
    clear_pinuse(n); set_size_and_pinuse_of_free_chunk(p, s)

// ------------------- Chunks sizes and alignments -----------------------

fun get CHUNK_OVERHEAD() U64
    return SIZE_T_SIZE
fun get MCHUNK_SIZE() U64
    return sizeof(mchunk).toU64Mask
fun get MIN_CHUNK_SIZE() U64
    return (MCHUNK_SIZE + CHUNK_ALIGN_MASK) & ~CHUNK_ALIGN_MASK
fun get MAX_REQUEST() U64
    return -MIN_CHUNK_SIZE << 2
fun get MIN_REQUEST() U64
    return MIN_CHUNK_SIZE - CHUNK_OVERHEAD - SIZE_T_ONE

// conversion from malloc headers to user pointers, and back
fun chunk2mem(p *Void) *Void
    return castPointer<*Void>(castPointer<*Byte>(p) + TWO_SIZE_T_SIZES)
fun mem2chunk(mem *Void) *mchunk
    return castPointer<*mchunk>(castPointer<*Byte>(mem) - TWO_SIZE_T_SIZES)
fun align_as_chunk(A *Byte) *mchunk
    return castPointer<*mchunk>(A + align_offset(chunk2mem(A)))
fun pad_request(req U64) U64
    return ((req) + CHUNK_OVERHEAD + CHUNK_ALIGN_MASK) & ~CHUNK_ALIGN_MASK
fun request2size(req U64) U64
    return ife req < MIN_REQUEST : MIN_CHUNK_SIZE : pad_request(req)

// ------------------ Operations on head and foot fields -----------------

//  The head field of a chunk is or'ed with PINUSE_BIT when previous
//  adjacent chunk in use, and or'ed with CINUSE_BIT if this chunk is in
//  use, unless mmapped, in which case both bits are cleared.
//  FLAG4_BIT is not used by this malloc, but might be useful in extensions.

const PINUSE_BIT U64 = SIZE_T_ONE
const CINUSE_BIT U64 = SIZE_T_TWO
const FLAG4_BIT U64 = SIZE_T_FOUR
const INUSE_BITS U64 = PINUSE_BIT | CINUSE_BIT
const FLAG_BITS U64 = PINUSE_BIT | CINUSE_BIT | FLAG4_BIT

// Head value for fenceposts
fun get FENCEPOST_HEAD() U64
    return INUSE_BITS | SIZE_T_SIZE


// ---------------------- Overlaid data structures -----------------------
//
//  When chunks are not in use, they are treated as nodes of either
//  lists or trees.
//
//  "Small"  chunks are stored in circular doubly-linked lists, and look
//  like this:
//
//    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Size of previous chunk                            |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//    `head:' |             Size of chunk, in bytes                         |P|
//      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Forward pointer to next chunk in list             |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Back pointer to previous chunk in list            |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Unused space (may be 0 bytes long)                .
//            .                                                               .
//            .                                                               |
//nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//    `foot:' |             Size of chunk, in bytes                           |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
//  Larger chunks are kept in a form of bitwise digital trees (aka
//  tries) keyed on chunksizes.  Because malloc_tree_chunks are only for
//  free chunks greater than 256 bytes, their size doesn't impose any
//  constraints on user chunk sizes.  Each node looks like:
//
//    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Size of previous chunk                            |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//    `head:' |             Size of chunk, in bytes                         |P|
//      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Forward pointer to next chunk of same size        |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Back pointer to previous chunk of same size       |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Pointer to left child (child[0])                  |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Pointer to right child (child[1])                 |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Pointer to parent                                 |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             bin index of this chunk                           |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//            |             Unused space                                      .
//            .                                                               |
//nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//    `foot:' |             Size of chunk, in bytes                           |
//            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
//  Each tree holding treenodes is a tree of unique chunk sizes.  Chunks
//  of the same size are arranged in a circularly-linked list, with only
//  the oldest chunk (the next to be used, in our FIFO ordering)
//  actually in the tree.  (Tree members are distinguished by a non-nil
//  parent pointer.)  If a chunk with the same size an an existing node
//  is inserted, it is linked off the existing node using pointers that
//  work in the same way as fd/bk pointers of small chunks.
//
//  Each tree contains a power of 2 sized range of chunk sizes (the
//  smallest is 0x100 <= x < 0x180), which is is divided in half at each
//  tree level, with the chunks in the smaller half of the range (0x100
//  <= x < 0x140 for the top nose) in the left subtree and the larger
//  half (0x140 <= x < 0x180) in the right subtree.  This is, of course,
//  done by inspecting individual bits.
//
//  Using these rules, each node's left subtree contains all smaller
//  sizes than its right subtree.  However, the node at the root of each
//  subtree has no particular ordering relationship to either.  (The
//  dividing line between the subtree sizes is based on trie relation.)
//  If we remove the last chunk of a given size from the interior of the
//  tree, we need to replace it with a leaf node.  The tree ordering
//  rules permit a node to be replaced by any leaf below it.
//
//  The smallest chunk in a tree (a common operation in a best-fit
//  allocator) can be found by walking a path to the leftmost leaf in
//  the tree.  Unlike a usual binary tree, where we follow left child
//  pointers until we reach a nil, here we follow the right child
//  pointer any time the left one is nil, until we reach a leaf with
//  both child pointers nil. The smallest chunk in the tree will be
//  somewhere along that path.
//
//  The worst case number of steps to add, find, or remove a node is
//  bounded by the number of bits differentiating chunks within
//  bins. Under current bin calculations, this ranges from 6 up to 21
//  (for 32 bit sizes) or up to 53 (for 64 bit sizes). The typical case
//  is of course much better.
//
// Originally malloc_tree_chunk, now tchunk
[pub] type tchunk
    // The first four fields must be compatible with mchunk
    prev_foot_ptr *Void
    head_ptr *Void
    fd *tchunk
    bk *tchunk

    child0 *tchunk
    child1 *tchunk
    parent *tchunk
    index U64

fun get prev_foot(my tchunk) U64
    return castPointer<U64>(my.prev_foot_ptr)
fun set prev_foot(my tchunk, value U64)
    my.prev_foot_ptr = castPointer<*Void>(Xuint(value))

fun get head(my tchunk) U64
    return castPointer<U64>(my.head_ptr)
fun set head(my tchunk, value U64)
    my.head_ptr = castPointer<*Void>(Xuint(value))

fun chunk_plus_offset(p *tchunk, s U64) *mchunk
    return castPointer<*mchunk>(castPointer<*Byte>(p) + s)
fun is_inuse(p *tchunk) Bool
    return (p.head & INUSE_BITS) != PINUSE_BIT
fun chunksize(p *tchunk) U64
    return p.head & ~FLAG_BITS
fun next_chunk(p *tchunk) *tchunk
    return castPointer<*tchunk>(castPointer<*Byte>(p) + (p.head & ~FLAG_BITS))
fun prev_chunk(p *tchunk) *tchunk
    return castPointer<*tchunk>(castPointer<*Byte>(p) - (p.prev_foot))


fun next_pinuse(p *tchunk) Bool
    return ((next_chunk(p).head) & PINUSE_BIT) != 0
fun leftmost_child(t *tchunk) *tchunk
    return ife t.child0 != nil : t.child0 : t.child1


// ----------------------------- Segments --------------------------------

//  Each malloc space may include non-contiguous segments, held in a
//  list headed by an embedded malloc_segment record representing the
//  top-most space. Segments also include flags holding properties of
//  the space. Large chunks that are directly allocated by mmap are not
//  included in this list. They are instead independently created and
//  destroyed without otherwise keeping track of them.
//
//  Segment management mainly comes into play for spaces allocated by
//  MMAP.  Any call to MMAP might or might not return memory that is
//  adjacent to an existing segment.  MORECORE normally contiguously
//  extends the current space, so this space is almost always adjacent,
//  which is simpler and faster to deal with. (This is why MORECORE is
//  used preferentially to MMAP when both are available -- see
//  sys_alloc.)  When allocating using MMAP, we don't use any of the
//  hinting mechanisms (inconsistently) supported in various
//  implementations of unix mmap, or distinguish reserving from
//  committing memory. Instead, we just ask for space, and exploit
//  contiguity when we get it.  It is probably possible to do
//  better than this on some systems, but no general scheme seems
//  to be significantly better.
//
//  Management entails a simpler variant of the consolidation scheme
//  used for chunks to reduce fragmentation -- new adjacent memory is
//  normally prepended or appended to an existing segment. However,
//  there are limitations compared to chunk consolidation that mostly
//  reflect the fact that segment processing is relatively infrequent
//  (occurring only when getting memory from system) and that we
//  don't expect to have huge numbers of segments:
//
//  * Segments are not indexed, so traversal requires linear scans.  (It
//    would be possible to index these, but is not worth the extra
//    overhead and complexity for most programs on most platforms.)
//  * New segments are only appended to old ones when holding top-most
//    memory; if they cannot be prepended to others, they are held in
//    different segments.
//
//  Except for the top-most segment of an mstate, each segment record
//  is kept at the tail of its segment. Segments are added by pushing
//  segment records onto the list headed by &mstate.seg for the
//  containing mstate.
//
//  Segment flags control allocation/merge/deallocation policies:
//  * If EXTERN_BIT set, then we did not allocate this segment,
//    and so should not try to deallocate or merge with others.
//    (This currently holds only for the initial segment passed
//    into create_mspace_with_base.)
//  * If USE_MMAP_BIT set, the segment may be merged with
//    other surrounding mmapped segments and trimmed/de-allocated
//    using munmap.
//  * If neither bit is set, then the segment was obtained using
//    MORECORE so can be merged with surrounding MORECORE'd segments
//    and deallocated/trimmed using MORECORE with negative arguments.
//
// malloc_segment
type msegment
    baseAddr *Byte      // base address
    size U64            // allocated size
    next *msegment      // ptr to next segment

//  malloc_params holds global properties, including those that can be
//  dynamically set using mallopt. There is a single instance, mparams,
//  initialized in init_mparams. Note that the non-zeroness of "magic"
//  also serves as an initialization flag.
type malloc_params
    magic U64
    page_size U64
    granularity U64
    trim_threshold U64